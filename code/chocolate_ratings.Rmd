---
title: "Chocolate ratings"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| include: false
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)

library(showtext)

font_add_google("IBM Plex Sans", "IBMPlexSans")
showtext_auto()
```

## Explore data

Exploratory data analysis (EDA) is an [important part of the modeling process](https://www.tmwr.org/software-modeling.html#model-phases).
```{r}
library(tidyverse)

url <- "https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-01-18/chocolate.csv"

chocolate <- read_csv(url)
```

```{r}
chocolate %>% 
  ggplot(aes(rating)) +
  geom_histogram(bins = 15)
```

```{r}
library(tidytext)

tidy_chocolate <- chocolate %>% 
  unnest_tokens(word, most_memorable_characteristics)

tidy_chocolate %>% count(word, sort = TRUE)
```

```{r}
tidy_chocolate %>% 
  group_by(word) %>% 
  summarise(
    n = n(),
    rating = mean(rating)
  ) %>% 
  ggplot(aes(n, rating)) +
  geom_hline(
    yintercept = mean(chocolate$rating), lty = 2,
    color = "gray50", size = 1.5
  ) +
  geom_jitter(color = "midnightblue", alpha = 0.7) +
  geom_text(aes(label = word),
            check_overlap = TRUE, family = "IBMPlexSans",
            vjust = "top", hjust = "left") +
  scale_x_log10()
```

## Build models

Let's consider how to [spend our data budget](https://www.tmwr.org/splitting.html):

- create training and testing sets
- create resampling folds from the *training* set

```{r}
library(tidymodels)

set.seed(123)
choco_split <- initial_split(chocolate, strata = rating)
choco_train <- training(choco_split)
choco_test <- testing(choco_split)

set.seed(234)
choco_folds <- vfold_cv(choco_train, strata = rating)
```

```{r}
choco_train %>% select(rating, most_memorable_characteristics)
```

```{r}
library(textrecipes)

choco_rec <- recipe(rating ~ most_memorable_characteristics, data = choco_train) %>% 
  step_tokenize(most_memorable_characteristics) %>% 
  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% 
  step_tfidf(most_memorable_characteristics)

# just to check this works
prep(choco_rec) %>% bake(new_data = NULL) ##%>% skimr::skim()
```

Let's create a [**model specification**](https://www.tmwr.org/models.html) for each model we want to try:

```{r}
rf_spec <- rand_forest(trees = 500) %>% 
  set_mode("regression")

svm_spec <- svm_linear() %>% 
  set_mode("regression")
```

To set up your modeling code, consider using the [parsnip addin](https://parsnip.tidymodels.org/reference/parsnip_addin.html) or the [usemodels](https://usemodels.tidymodels.org/) package.

Now let's build a [**model workflow**](https://www.tmwr.org/workflows.html) combining each model specification with a data preprocessor:

```{r}
svm_wf <- workflow(choco_rec, svm_spec)
rf_wf <- workflow(choco_rec, rf_spec)
```

If your feature engineering needs are more complex than provided by a formula like `sex ~ .`, use a [recipe](https://www.tidymodels.org/start/recipes/). [Read more about feature engineering with recipes](https://www.tmwr.org/recipes.html) to learn how they work.


## Evaluate models

These models have no tuning parameters so we can evaluate them as they are. [Learn about tuning hyperparameters here.](https://www.tidymodels.org/start/tuning/)

```{r}
doParallel::registerDoParallel()
contrl_preds <- control_resamples(save_pred = TRUE)

svm_rs <- fit_resamples(
  svm_wf,
  resamples = choco_folds,
  control = contrl_preds
)

ranger_rs <- fit_resamples(
  rf_wf,
  resamples = choco_folds,
  control = contrl_preds
)

```

How did these two models compare?

```{r}
collect_metrics(svm_rs)
collect_metrics(ranger_rs)
```

We can visualize these results using an ROC curve (or a confusion matrix via `conf_mat()`):

```{r}
bind_rows(
  collect_predictions(svm_rs) %>% 
    mutate(mod = "SVM"),
  collect_predictions(ranger_rs) %>% 
    mutate(mod = "ranger")
) %>% 
  ggplot(aes(rating, .pred, color = id))  +
  geom_abline(lty = 2, color = "gray50", size = 1.2) +
  geom_jitter(width = 0.5, alpha = 0.5) +
  facet_wrap(vars(mod)) +
  coord_fixed()
```

These models perform very similarly, so perhaps we would choose the simpler, linear model. The function `last_fit()` *fits* one final time on the training data and *evaluates* on the testing data. This is the first time we have used the testing data.

```{r}
final_fitted <- last_fit(svm_wf, choco_split)
collect_metrics(final_fitted)  ## metrics evaluated on the *testing* data
```

This object contains a fitted workflow that we can use for prediction.

```{r}
final_wf <- extract_workflow(final_fitted)
predict(final_wf, choco_test[55,])
```

You can save this fitted `final_wf` object to use later with new data, for example with `readr::write_rds()`.
```{r}
extract_workflow(final_fitted) %>% 
  tidy() %>%
  filter(term != "Bias") %>% 
  group_by(estimate > 0) %>% 
  slice_max(abs(estimate), n = 10) %>% 
  ungroup() %>% 
  mutate(term = str_remove(term, "tfidf_most_memorable_characteristics_")) %>%
  ggplot(aes(estimate, fct_reorder(term, estimate), fill = estimate > 0)) +
  geom_col(alpha = 0.8) +
  scale_fill_discrete(labels = c("low ratings", "high ratings")) +
  labs(y = NULL, fill = "More from ...")
```


