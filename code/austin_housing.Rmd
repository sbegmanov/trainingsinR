---
title: "austin_housing"
output: html_document
editor_options: 
  chunk_output_type: console
---
## https://www.kaggle.com/c/sliced-s01e11-semifinals/data?select=train.csv

```{r}
library(tidyverse)

train_raw <- read_csv("data/austin_train.csv")

problems()
skimr::skim(train_raw)

train_raw %>% View()
train_raw %>% count(priceRange)
```

```{r}
price_lot <- train_raw %>% 
  mutate(priceRange = parse_number(priceRange)) %>% 
  ggplot(aes(longitude, latitude, z = priceRange)) +
  stat_summary_hex(alpha = 0.8, bins = 50) +
  scale_fill_viridis_c() +
  labs(
    fill = "mean",
    title = "Price"
  )
```

```{r}
library(patchwork)

plot_austin <- function(var, title) {
  train_raw %>% 
    ggplot(aes(longitude, latitude, z = {{ var }})) +
    stat_summary_hex(alpha = 0.8, bins = 50) +
    scale_fill_viridis_c() +
    labs(
      fill = "mean",
      title = title
    )
}

plot_austin(avgSchoolRating, "School rating")

(price_lot + plot_austin(avgSchoolRating, "School rating")) / 
  (plot_austin(yearBuilt, "Year built") + plot_austin(log(lotSizeSqFt), "Lot size (log)"))

```

```{r}
library(tidytext)

train_raw %>% slice_sample(n = 5) %>% select(description)

austin_tidy <- train_raw %>% 
  mutate(priceRange = parse_number(priceRange) + 100000) %>%
  unnest_tokens(word, description) %>%
  anti_join(get_stopwords())

austin_tidy %>% 
  count(word, sort = TRUE)
```

```{r}
top_words <- austin_tidy %>% 
  count(word, sort = TRUE) %>% 
  filter(!word %in% as.character(1:5)) %>% 
  slice_max(n, n = 100) %>% 
  pull(word)

word_freqs <- austin_tidy %>% 
  count(word, priceRange) %>% 
  complete(word, priceRange, fill = list(n = 0)) %>%
  group_by(priceRange) %>% #group_keys()  # to check price range group
  mutate(
    price_total = sum(n),
    proportion = n / price_total
  ) %>% 
  ungroup() %>% 
  filter(word %in% top_words)
```

```{r}
library(broom)

word_mods <- word_freqs %>% 
  nest(data = -word) %>% 
  mutate(
    model = map(data, ~ glm(cbind(n, price_total) ~ priceRange, ., family = "binomial")),
    model = map(model, tidy)) %>% 
  unnest(model) %>% 
  filter(term == "priceRange") %>% 
  mutate(p.value = p.adjust(p.value)) %>% 
  arrange(-estimate)
```

```{r}
library(ggrepel)

word_mods %>% 
  ggplot(aes(estimate, p.value)) +
  geom_vline(xintercept = 0, lty = 2, alpha = 0.7, color = "gray50") +
  geom_point(color = "midnightblue", alpha = 0.8, size = 2.5) +
  scale_y_log10() +
  geom_text_repel(aes(label = word), family = "IBMPlexSans")
```

```{r}
higher_words <- word_mods %>% 
  filter(p.value < 0.05) %>% 
  slice_max(estimate, n = 12) %>% 
  pull(word)

lower_words <- word_mods %>% 
  filter(p.value < 0.05) %>% 
  slice_max(-estimate, n = 12) %>% 
  pull(word)
```

```{r}
word_freqs %>% 
  filter(word %in% lower_words) %>% 
  ggplot(aes(priceRange, proportion, colour = word)) +
  geom_line(size = 2.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(vars(word), scales = "free_y") +
  scale_x_continuous(labels = scales::dollar) +
  scale_y_continuous(labels = scales::percent, limits = c(0, NA)) +
  labs(
    x = NULL,
    y = "proportion of total words used  for homes at that price"
  ) +
  theme_light(
    base_family = "IBMPlexSans"
  )
```
## Build a model
```{r}
library(tidymodels)

train_raw %>% count(city, sort = TRUE)

set.seed(123)
austin_split <- train_raw %>% 
  select(-city) %>% 
  mutate(description = str_to_lower(description)) %>% 
  initial_split(strata = priceRange)

austin_train <- training(austin_split)
austin_test <- testing(austin_split)
# austin_metrics <- metric_set(accuracy, roc_auc, mn_log_loss)

set.seed(234)
austin_folds <- vfold_cv(austin_train, v = 5, strata = priceRange)
```

```{r}
higher_pat <- glue::glue_collapse(higher_words, sep = "|")
lower_pat <- glue::glue_collapse(lower_words, sep = "|")

austin_rec <- recipe(priceRange ~ ., data = austin_train) %>% 
  update_role(uid, new_role = "uid") %>% 
  step_regex(description, pattern = higher_pat, result = "high_price_words") %>% 
  step_regex(description, pattern = lower_pat, result = "low_price_words") %>% 
  step_rm(description) %>% 
  step_novel(homeType) %>% 
  step_unknown(homeType) %>%
  step_other(homeType, threshold = 0.02) %>% 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  # step_nzv(all_predictors())
  step_zv(all_predictors())

prep(austin_rec) %>% bake(new_data = NULL)
```

```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(),
  min_n = tune(),
  mtry = tune(),
  sample_size = tune(),
  learn_rate = tune()
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

# xgb_word_wf <- workflow(austin_rec, xgb_spec)
xgb_word_wf <- workflow() %>% 
  add_model(xgb_spec) %>% 
  add_recipe(austin_rec)

set.seed(123)
xgb_grid <- grid_max_entropy(
  tree_depth(c(5L, 10L)),
  min_n(c(10L, 40L)),
  mtry(c(5L, 10L)),
  sample_prop(c(0.5, 1.0)),
  learn_rate(c(-2, -1)),
  size = 20
)
```

```{r}
library(finetune)
doParallel::registerDoParallel()

set.seed(234)
xgb_word_rs <- tune_race_anova(
  xgb_word_wf,
  resamples = austin_folds,
  grid = xgb_grid,
  metrics = metric_set(mn_log_loss),
  control = control_race(verbose_elim = TRUE)
)
```

```{r}
# library(finetune)
# library(future)
# plan(multisession)
# 
# set.seed(234)
# xgb_word_rs <- tune_race_anova(
#   xgb_word_wf,
#   resamples = austin_folds,
#   grid = xgb_grid,
#   metrics = metric_set(mn_log_loss),
#   control = control_race(verbose_elim = TRUE)
# )
```
## Evaluate results
```{r}
plot_race(xgb_word_rs)
```

```{r}
show_best(xgb_word_rs)
```

```{r}
xgb_last <- xgb_word_wf %>% 
  finalize_workflow(select_best(xgb_word_rs, "mn_log_loss")) %>% 
  last_fit(austin_split)
```

```{r}
collect_predictions(xgb_last) %>% 
  mn_log_loss(priceRange, `.pred_0-250000`:`.pred_650000+`)
```

```{r}
collect_predictions(xgb_last) %>% 
  conf_mat(priceRange, .pred_class) %>% 
  autoplot()
```

```{r}
collect_predictions(xgb_last) %>% 
  roc_curve(priceRange, `.pred_0-250000`:`.pred_650000+`) %>% 
  ggplot(aes(1 - specificity, sensitivity, color = .level)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_path(alpha = 0.8, size = 1.2) +
  coord_equal() +
  labs(
    color = NULL
  )
```

```{r}
library(vip)

extract_workflow(xgb_last) %>% 
  extract_fit_parsnip() %>% 
  vip(geom = "point", num_features = 15)
```













