---
title: "squirrels"
format: html
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)

site_data <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-01-10/PFW_count_site_data_public_2021.csv') %>%
  mutate(squirrels = ifelse(squirrels, "squirrels", "no squirrels"))

site_data %>% count(squirrels)

glimpse(site_data)
```

```{r}
site_data %>% 
  filter(!is.na(squirrels)) %>% 
  group_by(squirrels) %>% 
  summarise(nearby_feeders = mean(nearby_feeders, na.rm = TRUE))
```

```{r}
# site_data %>% 
#   filter(!is.na(squirrels)) %>% 
#   group_by(squirrels) %>% 
#   summarise(across(contains("hab"), mean, na.rm = TRUE)) %>% 
#   pivot_longer(contains("hab")) %>% 
#   mutate(name = str_remove(name, "hab_")) %>% 
#   ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +
#   geom_col(alpha = 0.8, position = "dodge") +
#   scale_x_continuous(labels = scales::percent) +
#   labs(x = "% of locations", y = NULL, fill = NULL)

site_data %>% 
  filter(!is.na(squirrels)) %>% 
  group_by(squirrels) %>% 
  summarise(
    across(contains("hab"), \(x) mean(x, na.rm = TRUE))
  ) %>% 
  pivot_longer(contains("hab")) %>% 
   mutate(name = str_remove(name, "hab_")) %>% 
  ggplot(aes(value, fct_reorder(name, value), fill = squirrels)) +
  geom_col(alpha = 0.8, position = "dodge") +
  scale_x_continuous(labels = scales::percent) +
  labs(x = "% of locations", y = NULL, fill = NULL)
```
## Build a model
```{r}
library(tidymodels)

# site_data %>% filter(!is.na(squirrels)) %>% select(where(~all(is.na(.))))

set.seed(123)
feeder_split <- site_data %>% 
  filter(!is.na(squirrels)) %>% 
  select(where(~!all(is.na(.x)))) %>% 
  select(-loc_id, -proj_period_id, -fed_yr_round) %>% 
  select(squirrels, everything()) %>% 
  initial_split(strata = squirrels)

feeder_train <- training(feeder_split)
feeder_test <- testing(feeder_split)

glimpse(feeder_train)
feeder_train %>% count(squirrels)

set.seed(234)
feeder_folds <- vfold_cv(feeder_train, strata = squirrels)
```

```{r}
feeder_rec <-
  recipe(squirrels ~ ., data = feeder_train) %>% 
  step_impute_mean(all_numeric_predictors()) %>% 
  step_nzv(all_numeric_predictors())

prep(feeder_rec)
```

```{r}
glmnet_spec <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

```{r}
library(themis)

wf_set <- workflow_set(
  list(basic = feeder_rec, downsampling = feeder_rec %>% step_downsample(squirrels)),
  list(glmnet = glmnet_spec)
)
```

```{r}
narrower_penalty <- penalty(range = c(-3, 0))

doParallel::registerDoParallel()
set.seed(345)
tune_rs <- workflow_map(
  wf_set,
  "tune_grid",
  resamples = feeder_folds,
  grid = 15,
  metrics = metric_set(accuracy, mn_log_loss, sensitivity, specificity),
  param_info = parameters(narrower_penalty)
)
```
## Evaluate and finalize model
```{r}
autoplot(tune_rs) + theme(legend.position = "none")
rank_results(tune_rs, rank_metric = "sensitivity")
rank_results(tune_rs, rank_metric = "mn_log_loss")
```

```{r}
downsample_rs <-
  tune_rs %>% 
  extract_workflow_set_result("downsampling_glmnet")

autoplot(downsample_rs)
```

```{r}
best_penalty <- downsample_rs %>% 
  select_by_one_std_err(-penalty, metric = "mn_log_loss")
```

```{r}
final_fit <- wf_set %>% 
  extract_workflow("downsampling_glmnet") %>% 
  finalize_workflow(best_penalty) %>% 
  last_fit(feeder_split)
```

```{r}
collect_metrics(final_fit)

collect_predictions(final_fit) %>% 
  conf_mat(squirrels, .pred_class)
```

```{r}
library(vip)

feeder_vip <- extract_fit_engine(final_fit) %>% 
  vi()
```

```{r}
feeder_vip %>% 
  group_by(Sign) %>% 
  slice_max(Importance, n = 15) %>% 
  ungroup() %>% 
  ggplot(aes(Importance, fct_reorder(Variable, Importance), fill = Sign)) +
  geom_col() +
  facet_wrap(vars(Sign), scales = "free_y") +
  labs(y = NULL) +
  theme(legend.position = "none")
```










+

